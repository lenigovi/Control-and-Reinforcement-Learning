{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Decision Process (MDP) represents environments with state-spaces that allow an agent to make decisions. In search algorithms, we return a sequence of actions that lead to the goal state, and actions always translate to states. In reinforcement learning, we return a sequence of states that lead to the goal state.  To do this, the agent is rewarded for taking actions that lead to the goal state. MDP is a richer environment representation that allows randomness that other machine learning strategies cannot handle. MDP uses stochastic transition probabilities to model the environment. Stochastic transitions include randomness, for example, the agent might move in a direction and a condition might occur which puts the agent in an unmodeled condition. In robotics, the ability to maintain function in uncertainties is called resiliance. Therefore, MDP is an important tool for kinematic resiliance.\n",
    "\n",
    "Probability distributions include $s'$ what state we end up in, $s$ given the current state that we are in, and $a$ the action that we choose.\n",
    "\n",
    "$$Pr(s|s', a)=0.2$$\n",
    "\n",
    "This representation means three things: (i) the next state depends only one the previous state, (ii)  the probability function is the description and representation of the environment, (iii) the agent generates its own data.\n",
    "\n",
    "To determine how to acquire 'good' rewards, we propose a policy. A policy, represented with $\\pi$, is a function of states that maps states to actions, and tells which action to take in each state.\n",
    "\n",
    "A policy is different than a sequence of actions. In sequence of actions, if the transition takes the agent to an unmodeled or uncertain state, the algorithm runs out of options. However, policy can be evaluated in the uncertain state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a rectangular gridworld representation (see below) of a simple finite Markov Decision Process (MDP). The cells of the grid correspond to the states of the environment. At each cell, four actions are possible: north, south, east, and west, which deterministically cause the agent to move one cell in the respective direction on the grid. Actions that would take the agent off the grid leave its location unchanged, but also result in a reward of -1. In other words, rewards are positive for goals, negative for running into undesirable states, and zero the rest of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "\\hline\n",
    "0 & 0 & -1 & 0 \\\\\n",
    "\\hline\n",
    "\\colorbox{blue}{}& 0 & -1 & 1 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the rewards for the states. Those will be of +1 for the state that is desirable, of -1 for states that have to be avoided and of 0 for all other states. The Bellman equation expresses a relationship between the value of a state and the values of its successor states and must hold for each state for the value function $v_{\\pi}$\n",
    "\n",
    "$$ v_{\\pi}(s) = \\sum_a \\pi(a \\mid s) \\sum_{s'| r} p(s', r \\mid s, a) [ r + \\gamma v_{\\pi}(s')] $$\n",
    "\n",
    "where the actions, $a$ are taken from the set $A(s)$ that the next states, $s'$ are taken from the set $S$ and that the rewards, $r$ are taken from the set $R$.\n",
    "\n",
    "Suppose the agent selects all four actions with equal probability in all states. The figure below shows the value function, $v_{\\pi}$, for this policy, for the discounted reward case with $\\gamma=0.9$\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "0.42 & 0.48 & 0.52 & 0.63 \\\\\n",
    "\\hline\n",
    "0.38 & 0.43 & \\color{orange}{-1} & 0.72 \\\\\n",
    "\\hline\n",
    "0.34 & 0.38 & \\color{orange}{-1} & \\color{green}{+1} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Therefore the agent movement can be tracked so:\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    " & \\textcolor{lightblue}{\\rightarrow} & \\textcolor{lightblue}{\\rightarrow} & \\textcolor{lightblue}{\\downarrow} \\\\\n",
    "\\hline\n",
    "\\textcolor{lightblue}{\\rightarrow} & \\textcolor{lightblue}{\\uparrow} & \\textcolor{orange}{-1} & \\textcolor{lightblue}{\\downarrow} \\\\\n",
    "\\hline\n",
    "\\textcolor{lightblue}{\\uparrow\\rightarrow} & \\textcolor{lightblue}{\\uparrow} & \\textcolor{orange}{-1} & \\textcolor{green}{+1} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic can be seen in the code snippet below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "SMALL_ENOUGH = 0.005\n",
    "GAMMA = 0.9\n",
    "NOISE = 0.1\n",
    "\n",
    "#Define all states\n",
    "all_states=[]\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "            all_states.append((i,j))\n",
    "\n",
    "#Define rewards for all states\n",
    "rewards = {}\n",
    "for i in all_states:\n",
    "    if i == (1,2):\n",
    "        rewards[i] = -1\n",
    "    elif i == (2,2):\n",
    "        rewards[i] = -1\n",
    "    elif i == (2,3):\n",
    "        rewards[i] = 1\n",
    "    else:\n",
    "        rewards[i] = 0\n",
    "\n",
    "#Dictionary of possible actions. We have two \"end\" states (1,2 and 2,2)\n",
    "actions = {\n",
    "    (0,0):('D', 'R'),\n",
    "    (0,1):('D', 'R', 'L'),\n",
    "    (0,2):('D', 'L', 'R'),\n",
    "    (0,3):('D', 'L'),\n",
    "    (1,0):('D', 'U', 'R'),\n",
    "    (1,1):('D', 'R', 'L', 'U'),\n",
    "    (1,3):('D', 'L', 'U'),\n",
    "    (2,0):('U', 'R'),\n",
    "    (2,1):('U', 'L', 'R'),\n",
    "    }\n",
    "\n",
    "#Define an initial policy\n",
    "policy={}\n",
    "for s in actions.keys():\n",
    "    policy[s] = np.random.choice(actions[s])\n",
    "\n",
    "#Define initial value function\n",
    "V={}\n",
    "for s in all_states:\n",
    "    if s in actions.keys():\n",
    "        V[s] = 0\n",
    "    if s ==(2,2):\n",
    "        V[s]=-1\n",
    "    if s == (1,2):\n",
    "        V[s]=-1\n",
    "    if s == (2,3):\n",
    "        V[s]=1\n",
    "\n",
    "iteration = 0\n",
    "while True:\n",
    "    biggest_change = 0\n",
    "    for s in all_states:\n",
    "        if s in policy:\n",
    "\n",
    "            old_v = V[s]\n",
    "            new_v = 0\n",
    "\n",
    "            for a in actions[s]:\n",
    "                if a == 'U':\n",
    "                    nxt = [s[0]-1, s[1]]\n",
    "                if a == 'D':\n",
    "                    nxt = [s[0]+1, s[1]]\n",
    "                if a == 'L':\n",
    "                    nxt = [s[0], s[1]-1]\n",
    "                if a == 'R':\n",
    "                    nxt = [s[0], s[1]+1]\n",
    "\n",
    "                #Choose a new random action to do (transition probability)\n",
    "                random_1=np.random.choice([i for i in actions[s] if i != a])\n",
    "                if random_1 == 'U':\n",
    "                    act = [s[0]-1, s[1]]\n",
    "                if random_1 == 'D':\n",
    "                    act = [s[0]+1, s[1]]\n",
    "                if random_1 == 'L':\n",
    "                    act = [s[0], s[1]-1]\n",
    "                if random_1 == 'R':\n",
    "                    act = [s[0], s[1]+1]\n",
    "\n",
    "                #Calculate the value\n",
    "                nxt = tuple(nxt)\n",
    "                act = tuple(act)\n",
    "                v = rewards[s] + (GAMMA * ((1-NOISE)* V[nxt] + (NOISE * V[act])))\n",
    "                if v > new_v: #Is this the best action so far? If so, keep it\n",
    "                    new_v = v\n",
    "                    policy[s] = a\n",
    "\n",
    "       #Save the best of all actions for the state\n",
    "            V[s] = new_v\n",
    "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "   #See if the loop should stop now\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "        break\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration algorithm optimizes policies to navigate statespace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future Directions in Reinforcement Learning\n",
    "- Extrapolating the learning to uncertain states\n",
    "- Sample efficiency\n",
    "- Multi-agent RL evolution\n",
    "- Robustness"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
