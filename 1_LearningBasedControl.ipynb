{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning and Learning Based Control\n",
    "\n",
    "The text will define learning based control "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "Markov Decision Process (MDP) represents environments with state-spaces that allow an agent to make decisions. In search algorithms, we return a sequence of actions that lead to the goal state, and actions always translate to states. In reinforcement learning, we return a sequence of states that lead to the goal state.  To do this, the agent is rewarded for taking actions that lead to the goal state. MDP is a richer environment representation that allows randomness that other machine learning strategies cannot handle. MDP uses stochastic transition probabilities to model the environment. Stochastic transitions include randomness, for example, the agent might move in a direction and a condition might occur which puts the agent in an unmodeled condition. In robotics, the ability to maintain function in uncertainties is called resiliance. Therefore, MDP is an important tool for kinematic resiliance.\n",
    "\n",
    "Probability distributions include $s'$ what state we end up in, $s$ given the current state that we are in, and $a$ the action that we choose.\n",
    "\n",
    "$$Pr(s|s', a)=0.2$$\n",
    "\n",
    "This representation means three things: (i) the next state depends only one the previous state, (ii)  the probability function is the description and representation of the environment, (iii) the agent generates its own data.\n",
    "\n",
    "To determine how to acquire 'good' rewards, we propose a policy. A policy, represented with $\\pi$, is a function of states that maps states to actions, and tells which action to take in each state.\n",
    "\n",
    "A policy is different than a sequence of actions. In sequence of actions, if the transition takes the agent to an unmodeled or uncertain state, the algorithm runs out of options. However, policy can be evaluated in the uncertain state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a rectangular gridworld representation (see below) of a simple finite Markov Decision Process (MDP). The cells of the grid correspond to the states of the environment. At each cell, four actions are possible: north, south, east, and west, which deterministically cause the agent to move one cell in the respective direction on the grid. Actions that would take the agent off the grid leave its location unchanged, but also result in a reward of -1. In other words, rewards are positive for goals, negative for running into undesirable states, and zero the rest of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "\\hline\n",
    "0 & 0 & -1 & 0 \\\\\n",
    "\\hline\n",
    "\\colorbox{blue}{}& 0 & -1 & 1 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the rewards for the states. Those will be of +1 for the state that is desirable, of -1 for states that have to be avoided and of 0 for all other states. The Bellman equation expresses a relationship between the value of a state and the values of its successor states and must hold for each state for the value function $v_{\\pi}$\n",
    "\n",
    "$$ v_{\\pi}(s) = \\sum_a \\pi(a \\mid s) \\sum_{s'| r} p(s', r \\mid s, a) [ r + \\gamma v_{\\pi}(s')] $$\n",
    "\n",
    "where the actions, $a$ are taken from the set $A(s)$ that the next states, $s'$ are taken from the set $S$ and that the rewards, $r$ are taken from the set $R$.\n",
    "\n",
    "Suppose the agent selects all four actions with equal probability in all states. The figure below shows the value function, $v_{\\pi}$, for this policy, for the discounted reward case with $\\gamma=0.9$\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "0.42 & 0.48 & 0.52 & 0.63 \\\\\n",
    "\\hline\n",
    "0.38 & 0.43 & \\color{orange}{-1} & 0.72 \\\\\n",
    "\\hline\n",
    "0.34 & 0.38 & \\color{orange}{-1} & \\color{green}{+1} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Therefore the agent movement can be tracked so:\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    " & \\textcolor{lightblue}{\\rightarrow} & \\textcolor{lightblue}{\\rightarrow} & \\textcolor{lightblue}{\\downarrow} \\\\\n",
    "\\hline\n",
    "\\textcolor{lightblue}{\\rightarrow} & \\textcolor{lightblue}{\\uparrow} & \\textcolor{orange}{-1} & \\textcolor{lightblue}{\\downarrow} \\\\\n",
    "\\hline\n",
    "\\textcolor{lightblue}{\\uparrow\\rightarrow} & \\textcolor{lightblue}{\\uparrow} & \\textcolor{orange}{-1} & \\textcolor{green}{+1} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic can be seen in the code snippet below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "SMALL_ENOUGH = 0.005\n",
    "GAMMA = 0.9\n",
    "NOISE = 0.1\n",
    "\n",
    "#Define all states\n",
    "all_states=[]\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "            all_states.append((i,j))\n",
    "\n",
    "#Define rewards for all states\n",
    "rewards = {}\n",
    "for i in all_states:\n",
    "    if i == (1,2):\n",
    "        rewards[i] = -1\n",
    "    elif i == (2,2):\n",
    "        rewards[i] = -1\n",
    "    elif i == (2,3):\n",
    "        rewards[i] = 1\n",
    "    else:\n",
    "        rewards[i] = 0\n",
    "\n",
    "#Dictionary of possible actions. We have two \"end\" states (1,2 and 2,2)\n",
    "actions = {\n",
    "    (0,0):('D', 'R'),\n",
    "    (0,1):('D', 'R', 'L'),\n",
    "    (0,2):('D', 'L', 'R'),\n",
    "    (0,3):('D', 'L'),\n",
    "    (1,0):('D', 'U', 'R'),\n",
    "    (1,1):('D', 'R', 'L', 'U'),\n",
    "    (1,3):('D', 'L', 'U'),\n",
    "    (2,0):('U', 'R'),\n",
    "    (2,1):('U', 'L', 'R'),\n",
    "    }\n",
    "\n",
    "#Define an initial policy\n",
    "policy={}\n",
    "for s in actions.keys():\n",
    "    policy[s] = np.random.choice(actions[s])\n",
    "\n",
    "#Define initial value function\n",
    "V={}\n",
    "for s in all_states:\n",
    "    if s in actions.keys():\n",
    "        V[s] = 0\n",
    "    if s ==(2,2):\n",
    "        V[s]=-1\n",
    "    if s == (1,2):\n",
    "        V[s]=-1\n",
    "    if s == (2,3):\n",
    "        V[s]=1\n",
    "\n",
    "iteration = 0\n",
    "while True:\n",
    "    biggest_change = 0\n",
    "    for s in all_states:\n",
    "        if s in policy:\n",
    "\n",
    "            old_v = V[s]\n",
    "            new_v = 0\n",
    "\n",
    "            for a in actions[s]:\n",
    "                if a == 'U':\n",
    "                    nxt = [s[0]-1, s[1]]\n",
    "                if a == 'D':\n",
    "                    nxt = [s[0]+1, s[1]]\n",
    "                if a == 'L':\n",
    "                    nxt = [s[0], s[1]-1]\n",
    "                if a == 'R':\n",
    "                    nxt = [s[0], s[1]+1]\n",
    "\n",
    "                #Choose a new random action to do (transition probability)\n",
    "                random_1=np.random.choice([i for i in actions[s] if i != a])\n",
    "                if random_1 == 'U':\n",
    "                    act = [s[0]-1, s[1]]\n",
    "                if random_1 == 'D':\n",
    "                    act = [s[0]+1, s[1]]\n",
    "                if random_1 == 'L':\n",
    "                    act = [s[0], s[1]-1]\n",
    "                if random_1 == 'R':\n",
    "                    act = [s[0], s[1]+1]\n",
    "\n",
    "                #Calculate the value\n",
    "                nxt = tuple(nxt)\n",
    "                act = tuple(act)\n",
    "                v = rewards[s] + (GAMMA * ((1-NOISE)* V[nxt] + (NOISE * V[act])))\n",
    "                if v > new_v: #Is this the best action so far? If so, keep it\n",
    "                    new_v = v\n",
    "                    policy[s] = a\n",
    "\n",
    "       #Save the best of all actions for the state\n",
    "            V[s] = new_v\n",
    "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "   #See if the loop should stop now\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "        break\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise - 3\n",
    "\n",
    "Task 1\n",
    "\n",
    "a) Give $v_\\pi (s)$ in terms of $q\\pi (s,a)$\n",
    "\n",
    "\n",
    "b) Give $\\pi_* (s,a)$ in terms of $q_*(s,a)$\n",
    "\n",
    "\n",
    "\n",
    "### Task 2: Reward Offset in Non-terminating Trajectories\n",
    "Show that this measure only adds a factor $v_c$, to the state values and hence does not change the  optimal behavior. Express $v_c$ in terms of $c$ and $\\gamma$\n",
    "\n",
    "$v_\\pi(s) = \\mathbb{E}_\\pi[G_t|S_t=s] = \\mathbb{E}_\\pi \\left[ \\sum_{k}^{\\infty} \\gamma^k R_(t+k+1) | S_t = s \\right]$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 3 Action Value Function\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Bellman Optimality Equations\n",
    "\n",
    "![image](img/recycling_bot.png){width=40%}\n",
    "\n",
    "a) Recycling Bot: Derive the Bellman optimality equations of the recycling robot and solve them for the optimal state value function.\n",
    "\n",
    "$ v_*(s) = max_{a} q_*(s,a) $ \\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $max {p(h,r_s | h,s) [r_s + \\gamma v_* (h)] } $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "\n",
    "# MDP Parameters\n",
    "R_S = 3\n",
    "R_W = 1\n",
    "R_E = -30\n",
    "\n",
    "ALPHA = 0.9\n",
    "BETA = 0.5\n",
    "GAMMA = 0.9\n",
    "\n",
    "\n",
    "# Forumulation of Bellman Optimality Equation\n",
    "def bell_opt_eq_rb(v):\n",
    "    v_h = v[0]\n",
    "    v_l = v[1]\n",
    "\n",
    "    F = np.empty(2)\n",
    "    F[0] = v_h - max(R_S + GAMMA*(ALPHA*v_h + (1-ALPHA)*v_l), R_W + GAMMA * v_h)\n",
    "    F[1] = v_l - max(BETA*R_S + R_E*(1-BETA) + GAMMA*((1-BETA)*v_h + BETA*v_l), R_W + GAMMA*v_l, GAMMA*v_h)\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "Value iteration algorithm optimizes policies to navigate statespace.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "while True:\n",
    "    biggest_change = 0\n",
    "    for s in all_states:\n",
    "        if s in policy\n",
    "\n",
    "            old_v = V[s]\n",
    "            new_v = 0\n",
    "\n",
    "            for a in actions[s]:\n",
    "                if a == 'U'       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty\n",
    "\n",
    "Markov Decision Processes often assume a fully modeled probability state model. If behavior leads outside of the model, some form of exception handling needs to be conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive State Estimation in Probabilistic Robotics\n",
    "\n",
    "The theory was suggested by Sebastian Thrun et.al, one of the most important parts about Markov Decision Processes is the idea that you do not need to know where your coming from but only your current state, the transition between states is what is modeled and the resulting state, not a whole history sequence of states within each state. Probabilistic Robotics mostly focuses on the problem of simultaneous localization and mapping for mobile robotics. The fundamental ideas within Recursive State Estimation are transferable, there are a number of algorithm to choose from most important for the state modeling are variations of what is called a particle filter (there quite a few different types) and Gaussian Mixture Models (sometimes referred to as multi hypothesis Kalman filter). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "This section will include Monte Carlo tree search, Bayesian Filter, trade-off factor between exploration and exploitation, and Upper Confidence Trees.\n",
    "\n",
    "\n",
    "### Monte Carlo Tree Search\n",
    "In an environment in which either complete modelling is not possible or decision tree is still \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Directions in Reinforcement Learning\n",
    "- Extrapolating the learning to uncertain states\n",
    "- Sample efficiency\n",
    "- Multi-agent RL evolution\n",
    "- Robustness"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
